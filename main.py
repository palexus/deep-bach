import streamlit as st
import base64
from utils import Choral_data, Seed_data, first_n_bars, save_piece
from prediction import auto_generate


header = st.container()
explanation = st.container()
generating = st.container()
preprocess = st.container()



with st.sidebar:
    radio = st.radio(
         "Navigation",
        ("Home", "The Model", "The Dataset", "Preprocessing", "Music Generation", "Examples", "Test")
    )

    if radio == "Music Generation":
        add_selectbox = st.sidebar.selectbox(
            "Choose one model",
            ("GPT3", "GPT2")
            )

        pushed = st.button("Generate seed!")
        main = st.checkbox("Generate music!")


if radio =="Home":
    with header:
        st.title('Curie cooks Bach')
        st.image("images/Johann_Sebastian_Bach.jpg")
        
elif radio == "The Model":
    st.title("GPT-3")
    #st.write("Using the OpenAI Curie model (GPT-3) to generate polyphonic Music\
    #        in the style of the Bach Chorales.")
    st.markdown("""
        * Generative Pre-trained Transformer 3 is an autoregressive language model that uses deep learning to produce human-like text.

        * created by OpenAI, a San Francisco-based artificial intelligence research laboratory, introduced in July 2020
        
        * GPT-3's full version has a capacity of 175 billion machine learning parameters

        * The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks. 
        
        * David Chalmers, an Australian philosopher: "one of the most interesting and important AI systems ever produced."

        * Microsoft announced on September 22, 2020, that it had licensed "exclusive" use of GPT-3 
        
        * others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.

        * An April 2022 review in The New York Times described GPT-3's capabilities as being able to write original prose with fluency equivalent to that of a human.""")

    st.header("The Curie model")
    st.markdown("""
        * Curie is extremely powerful, yet very fast (lower cost than davinci)
        * While Davinci is stronger in analyzing complicated text, Curie is quite capable for many nuanced tasks (sentiment classification, summarization) 
        * Curie is also quite good at answering questions and performing Q&A and as a general service chatbot.
        """)

elif radio == "The Dataset":
    with explanation:
        st.header("What are Bach chorales?")
        st.markdown("""
                    * The JSB chorales are a set of short, four-voice pieces of music 
                    * well-noted for their stylistic homogeneity
                    * composed from pre-existing melodies from contemporary Lutheran hymns 
                    * harmonised by remaining three voices. 
                    * A typical Bach chorale looks like this:""")
        st.image("images/Bach_example.jpg")
        st.header("Where can I get them?")
        st.markdown("""
            * There are freely available and are contained in many existing datasets
            * There are even fake Bach Choral datasets [JSFakes](https://github.com/omarperacha/js-fakes) which are composed by machines.
            * We use the python library [Music21](https://web.mit.edu/music21/). It includes the Bach Chorales.
        """)

elif radio == "Preprocessing":
    with preprocess:
        st.title("Data preprocessing")
        st.markdown("""
            * Quantize the Chorales to 16th time grid
            * filter out all chorales which dont have four voices or do not fit to the time grid
            * notate each voice as a sequence of pitches, use "_" if note is held and "r" for rest
        """)
        st.image("images/melody_example2.png")
        st.markdown("""
            * Since we have four voices we have four sequences. Write them from top to botton
            * Transpose the pieces to C Major / A minor
        """)
        st.image("images/encoding.png")
        st.header("GPT-3")
        st.markdown("""
            * not much work is from now on needed. Use the API to preprocess and fine-tune the model.
            * Use the following jsonlines to generate one object like this
        """)
        st.markdown("""
            ```
            {"prompt": "<prompt text>", "completion": "<ideal generated text>"}
            {"prompt": "<prompt text>", "completion": "<ideal generated text>"}
            {"prompt": "<prompt text>", "completion": "<ideal generated text>"}
            ...```  
        """)
        st.markdown("""
            * use the empty prompt technique, like [Haiku](https://beta.openai.com/docs/guides/fine-tuning/case-study-haikus-or-generation-of-content-which-fits-a-particular-format)
            ```  {prompt: "", completion: "< one Bach choral >"} ```   
        """)
        st.header("GPT-2")
        st.markdown("""
            * Use sequences of fixed length (for example 64 lines are four bars) as input
            * Use the line which comes after it as target
            * use the tokenizer of GPT-2 to create a sequence of numbers
        """)

    
elif radio == "Music Generation":
    with generating:
        st.header("The seed generator")         
        st.write("""The best way to use the model is to give some
            starting sequence. One Bar of notes gives good results. 
            Here is one example of a Bach chorale.""")
        
        
        if pushed:
            seed = Seed_data()
            seed.load_seed()
            seed.create_mp3()
            print("---------hello--------")
            print(seed.path_mp3)
            with open(seed.path_mp3, "rb") as fp:
                audio_bytes = fp.read()
            st.audio(audio_bytes, format="mp3")
            seed.to_pdf()
            if main:
                print("----in the main-------")
                input = first_n_bars(seed.choral, 1)
                output = auto_generate(input, 64)
                print(output)
                choral_data = Choral_data(output)
                choral_data.create_mp3()
                with open(choral_data.path_mp3, "rb") as fp:
                    audio_bytes = fp.read()
                st.audio(audio_bytes, format="mp3")
                choral_data.to_pdf()





elif radio == "Examples":
    st.title("Examples")
    with open("example/example_1.mp3", "rb") as fp:
        audio_bytes = fp.read()
        st.audio(audio_bytes, format="mp3")

    with open("example/example_1.pdf", "rb") as f:
        base64_pdf = base64.b64encode(f.read()).decode('utf-8')
        pdf_display = F'<iframe src="data:application/pdf;base64,{base64_pdf}" width="700" height="1000" type="application/pdf"></iframe>'
        st.markdown(pdf_display, unsafe_allow_html=True)

    with open("example/example_2.mp3", "rb") as fp:
        audio_bytes = fp.read()
        st.audio(audio_bytes, format="mp3")

    with open("example/example_2.pdf", "rb") as f:
        base64_pdf = base64.b64encode(f.read()).decode('utf-8')
        pdf_display = F'<iframe src="data:application/pdf;base64,{base64_pdf}" width="700" height="1000" type="application/pdf"></iframe>'
        st.markdown(pdf_display, unsafe_allow_html=True)
